{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5260892d",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6853ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ebd013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, num_features):\n",
    "        self.num_features = num_features\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "        self.activation_function = lambda x: 1 if x >= 0 else 0\n",
    "\n",
    "    def predict(self, x):\n",
    "        linear_output = np.dot(self.weights, x) + self.bias\n",
    "        print(np.dot(self.weights, x)+ self.bias)\n",
    "        return self.activation_function(linear_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6492a6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "Perceptron prediction: 1\n"
     ]
    }
   ],
   "source": [
    "# Create a perceptron with 2 input features and using the step function as the activation function\n",
    "perceptron = Perceptron(2)\n",
    "\n",
    "# Set the weights and bias\n",
    "perceptron.weights = np.array([0.5, -0.5])\n",
    "perceptron.bias = 0.2\n",
    "\n",
    "# Make predictions\n",
    "x = np.array([0.3, -0.7])\n",
    "prediction = perceptron.predict(x)\n",
    "print(\"Perceptron prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985e54c",
   "metadata": {},
   "source": [
    "# MLP (Multi Layer Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec838813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activation='sigmoid'):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        print(layer_sizes)\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            weight_matrix = np.random.randn(layer_sizes[i], layer_sizes[i+1])\n",
    "            self.weights.append(weight_matrix)\n",
    "            bias_vector = np.zeros(layer_sizes[i+1])\n",
    "            print(layer_sizes[i+1])\n",
    "            print(\"----------------\")\n",
    "            print(bias_vector)\n",
    "            print(\"-------------\")\n",
    "            self.biases.append(bias_vector)\n",
    "        \n",
    "        print(self.weights)\n",
    "        print(self.biases)\n",
    "\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation_function = self._sigmoid\n",
    "        elif activation == 'tanh':\n",
    "            self.activation_function = self._tanh\n",
    "        elif activation == 'relu':\n",
    "            self.activation_function = self._relu\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        activations = [x]\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            linear_output = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            print(\"************* hey\")\n",
    "            print(linear_output)\n",
    "            print(activations[-1])\n",
    "            print(\"////////////////finish\")\n",
    "            output = self.activation_function(linear_output)\n",
    "            activations.append(output)\n",
    "\n",
    "        return activations[-1]\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def _tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def _relu(self, x):\n",
    "        return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2841131a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 6, 3]\n",
      "8\n",
      "----------------\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "-------------\n",
      "6\n",
      "----------------\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "-------------\n",
      "3\n",
      "----------------\n",
      "[0. 0. 0.]\n",
      "-------------\n",
      "[array([[-0.49524029,  1.88981953, -0.51382169, -0.02348304,  1.47076002,\n",
      "        -1.01001069,  2.07349417,  0.01511628],\n",
      "       [ 0.7326151 , -0.17311577,  0.65140944,  2.01805801, -1.47546763,\n",
      "         0.9704011 , -1.94126797, -1.65907008],\n",
      "       [-1.38153925, -1.39201252,  1.35145324,  1.81354106,  0.43543916,\n",
      "        -0.6844334 , -1.07487883, -2.97702599],\n",
      "       [ 0.62863966,  0.0107043 ,  0.21020409, -0.01131568, -0.27206221,\n",
      "        -1.00073937,  0.01472577,  1.37963309]]), array([[ 0.48405106,  1.18495779, -0.44263087, -0.45964798,  1.48800607,\n",
      "         0.51703048],\n",
      "       [ 1.81630233, -1.72326383, -1.1016762 , -0.20249709, -0.87392201,\n",
      "         2.09056889],\n",
      "       [-1.16235478, -0.54037351,  0.14062633, -0.3928467 , -0.2192747 ,\n",
      "        -1.09303296],\n",
      "       [ 0.11730326,  0.53536662, -1.11335479, -0.27114656, -1.07160918,\n",
      "        -1.09082189],\n",
      "       [-0.86232262, -0.5584182 ,  0.58996273,  0.2672298 , -0.82242833,\n",
      "        -2.14136977],\n",
      "       [ 0.12444106,  0.69951052, -1.41074524,  0.25662564, -0.39006175,\n",
      "         0.49397976],\n",
      "       [ 0.9639057 ,  0.02569582, -0.89857251, -0.61131941,  0.60573868,\n",
      "        -0.5349525 ],\n",
      "       [ 1.03805275, -0.61919901, -0.79897361, -0.21129317, -0.62008607,\n",
      "         0.55498686]]), array([[-0.93893082, -1.46977207,  0.31282305],\n",
      "       [-1.01966066,  1.55096798,  2.63763535],\n",
      "       [ 0.16375582, -0.18194593,  0.8102439 ],\n",
      "       [-1.12998551, -0.57250015,  1.20754843],\n",
      "       [-0.12253316,  0.63527949, -0.21033866],\n",
      "       [-0.94530307,  0.02584757, -1.3709972 ]])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0.]), array([0., 0., 0.])]\n",
      "************* hey\n",
      "[[ 2.8192      1.84074164 -1.52556307 -2.88622991 -0.75888712 -1.17544736\n",
      "   1.8540323   6.87542422]\n",
      " [-1.07172468  2.32013237 -0.81388902 -1.624952    3.18677386 -3.33469162\n",
      "   4.3617115   2.2836114 ]\n",
      " [ 0.31035987  2.20746071 -0.12604207  2.33358936 -0.2386908   0.96445225\n",
      "  -0.05107483 -2.50507859]\n",
      " [ 1.29932566 -2.33171119  0.10907702 -1.06613537 -2.30898376  1.76916443\n",
      "  -2.5146043   1.54897118]\n",
      " [ 0.0330208  -2.29722034  1.63618652  2.15893366 -1.53620069  0.30283949\n",
      "  -3.05021822 -2.19729352]\n",
      " [-0.4028998  -0.91399998 -1.71580294 -5.38411759  1.33231112 -0.13867556\n",
      "   2.46019124  4.8177352 ]\n",
      " [-4.04048148 -0.59040259 -0.28540619 -0.94387144  3.46554007  0.42792006\n",
      "   2.07915508 -4.20593728]\n",
      " [-0.58969922  2.32148103 -0.83849313 -0.68487311  2.0094175  -1.46092698\n",
      "   2.96482561  0.80978731]\n",
      " [ 2.643077    0.5366429  -0.69953503  0.62073695 -3.31235954  3.67065727\n",
      "  -2.58015469  0.33049746]\n",
      " [ 0.84333239  1.22734986 -2.12877405 -3.87489993  0.42784752  0.84893927\n",
      "   2.29199405  4.22332501]]\n",
      "[[-0.0248919  -0.24115451 -1.31183624  1.86305889]\n",
      " [ 1.29214556 -0.97947338  0.21645635  0.93029095]\n",
      " [ 1.14001696  1.35628965 -0.21180356 -0.6542874 ]\n",
      " [-1.57891162 -0.14146307 -0.45092299 -0.00309368]\n",
      " [-0.69012036  0.47304682  0.657636    0.40282871]\n",
      " [-1.12606226 -2.13546966 -0.60949589 -0.3788122 ]\n",
      " [ 0.27054075 -1.31988052  0.93542238 -2.62027957]\n",
      " [ 1.14877582 -0.25778659 -0.07527514  0.10194018]\n",
      " [-0.53889999  1.47065037 -1.30621841 -0.80462734]\n",
      " [-0.30774747 -0.85018922 -1.19795354 -0.54281483]]\n",
      "////////////////finish\n",
      "************* hey\n",
      "[[ 3.44762915 -1.04665729 -3.12188325 -1.28691559  0.10422566  1.56481342]\n",
      " [ 2.50962401 -2.39344152 -2.35030566 -0.9967754  -1.42766421 -0.54367985]\n",
      " [ 1.73613313 -0.40778143 -3.45656924 -0.8891464  -1.40489118 -0.12939664]\n",
      " [ 0.91499836  0.6690938  -2.53451461 -0.63227602 -0.17197651  0.38181662]\n",
      " [-0.38956129  0.71515992 -2.03343864 -0.67786527 -0.87238633 -1.5028388 ]\n",
      " [ 1.83049099 -0.80521803 -2.28643103 -0.74514751 -0.58159431 -0.77288758]\n",
      " [ 0.30118626 -0.78013874 -1.74436204 -0.45667454 -1.18447176 -2.2650686 ]\n",
      " [ 2.41320811 -1.89502388 -2.64474679 -1.00142413 -1.34218371 -0.52719126]\n",
      " [ 2.05148254  0.49168518 -3.66903245 -0.76999658 -0.65947649  1.42136328]\n",
      " [ 3.08550009 -0.98585007 -3.40354534 -0.94721154 -0.51263009  0.95061716]]\n",
      "[[0.94370458 0.8630364  0.17864379 0.05283848 0.31888793 0.23587176\n",
      "  0.86459985 0.99896821]\n",
      " [0.25507524 0.91053072 0.30706239 0.16452306 0.96033351 0.03440005\n",
      "  0.98740414 0.90751062]\n",
      " [0.5769731  0.90091749 0.46853113 0.91162095 0.44060901 0.72401233\n",
      "  0.48723407 0.07550292]\n",
      " [0.78572147 0.08853049 0.52724225 0.25613872 0.09038166 0.85435373\n",
      "  0.07484068 0.82476509]\n",
      " [0.50825445 0.09135343 0.83701537 0.89650065 0.17708826 0.57513651\n",
      "  0.04520805 0.0999938 ]\n",
      " [0.40061583 0.28618202 0.15241256 0.00456793 0.79122266 0.46538656\n",
      "  0.92130353 0.99197977]\n",
      " [0.01728498 0.35654249 0.42912888 0.28011899 0.96969121 0.60537689\n",
      "  0.88886059 0.01468786]\n",
      " [0.35670387 0.91064053 0.30185224 0.33517454 0.88178231 0.18832559\n",
      "  0.95095953 0.69206418]\n",
      " [0.93358301 0.63103113 0.33191533 0.65038614 0.03514961 0.97517237\n",
      "  0.0704266  0.58188041]\n",
      " [0.69916659 0.7733544  0.10633143 0.02033435 0.60535956 0.70034458\n",
      "  0.90821182 0.98556167]]\n",
      "////////////////finish\n",
      "************* hey\n",
      "[[-2.25880221 -0.7973992   0.03957978]\n",
      " [-1.61496436 -1.26726088  0.36175198]\n",
      " [-1.99597287 -0.66512707  1.01340771]\n",
      " [-2.34260164  0.06993838  1.53522862]\n",
      " [-1.63362662  0.42612761  2.08648182]\n",
      " [-1.81530035 -0.75267663  1.03936659]\n",
      " [-1.39153925 -0.45509471  1.41933638]\n",
      " [-1.66399724 -1.17101162  0.45748896]\n",
      " [-2.6214896  -0.28832835  1.14006123]\n",
      " [-2.213153   -0.8933161   0.31179813]]\n",
      "[[0.96916036 0.25986751 0.04221356 0.21637534 0.52603285 0.82704296]\n",
      " [0.92481375 0.08367418 0.08704148 0.26957589 0.19346289 0.36733197]\n",
      " [0.85019523 0.39944421 0.03057355 0.29128601 0.1970411  0.4676959 ]\n",
      " [0.71402189 0.66130022 0.07347372 0.34699464 0.45711153 0.59431117]\n",
      " [0.40382292 0.67154031 0.11573654 0.33673792 0.294758   0.18200251]\n",
      " [0.86182021 0.30891044 0.09225299 0.32187955 0.35856583 0.3158548 ]\n",
      " [0.57473248 0.31428998 0.14875972 0.38777501 0.23424912 0.09405758]\n",
      " [0.91782896 0.13067271 0.06631353 0.26866151 0.20715118 0.37117222]\n",
      " [0.88609734 0.62050334 0.024867   0.31647985 0.34085722 0.80555205]\n",
      " [0.95629066 0.27173254 0.03218485 0.27944595 0.37457717 0.72123928]]\n",
      "////////////////finish\n",
      "[[0.0945929  0.31058213 0.50989365]\n",
      " [0.16590052 0.2197265  0.58946447]\n",
      " [0.11962639 0.33958883 0.73368651]\n",
      " [0.08765563 0.51747747 0.82277004]\n",
      " [0.16333416 0.6049486  0.88958232]\n",
      " [0.13999875 0.32023836 0.73872777]\n",
      " [0.19916214 0.38815014 0.80523436]\n",
      " [0.15922614 0.23667218 0.61241832]\n",
      " [0.06776813 0.42841316 0.75769088]\n",
      " [0.09857555 0.29042598 0.5773241 ]]\n",
      "********************\n",
      "[[ 0.00385429  2.08633966  0.57858991 -0.63326722]\n",
      " [ 0.46463022  0.49283337 -1.98254649 -0.64765615]\n",
      " [ 0.41083331 -1.59746036 -1.43667812 -0.17746375]\n",
      " [ 0.64175604 -0.4448934   0.75878086  0.44116938]\n",
      " [ 0.86180387  0.79897414 -0.05580519 -0.68108143]\n",
      " [ 0.2264172   0.52822434 -0.56545109 -0.34314373]\n",
      " [-0.05893405  1.98710327  0.56366197  0.28324413]\n",
      " [ 0.54030099 -0.70692032 -0.48820996 -0.14085533]\n",
      " [ 0.5425093   0.84661242 -0.79912632  0.41255566]\n",
      " [-0.14016785 -0.06653483 -0.0639815  -0.88446171]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "mlp = MLP(input_size=4, hidden_sizes=[8, 6], output_size=3, activation='sigmoid')\n",
    "input_data = np.random.randn(10, 4)  # Example input data with 10 samples\n",
    "output = mlp.forward(input_data)  # Compute the MLP's output\n",
    "print(output)\n",
    "print(\"********************\")\n",
    "print(np.random.randn(10, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e4f81c",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86600d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Initialize weights with random values\n",
    "        self.W1 = np.random.randn(input_size, hidden_size)\n",
    "        self.W2 = np.random.randn(hidden_size, output_size)\n",
    "\n",
    "#          # Initialize bases with random values\n",
    "#         self.b1 = np.random.randn(hidden_size)\n",
    "#         self.b2 = np.random.randn(output_size)\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.hidden_layer = self.sigmoid(np.dot(X, self.W1)) # + self.b1)\n",
    "        self.output_layer = self.sigmoid(np.dot(self.hidden_layer, self.W2)) # + self.b2)\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        # Calculate the gradients\n",
    "        output_error = y - self.output_layer\n",
    "        output_delta = output_error * self.sigmoid_derivative(self.output_layer)\n",
    "        hidden_error = output_delta.dot(self.W2.T)\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_layer)\n",
    "\n",
    "        # Update the weights\n",
    "        self.W2 -= self.hidden_layer.T.dot(output_delta) * learning_rate\n",
    "        self.W1 -= X.T.dot(hidden_delta) * learning_rate\n",
    "        \n",
    "#         self.b2 -= self.hidden_layer.T.dot(output_delta) * learning_rate\n",
    "#         self.b1 -= X.T.dot(hidden_delta) * learning_rate\n",
    "\n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.01):\n",
    "        for epoch in range(epochs):\n",
    "            self.forward(X)\n",
    "            self.backward(X, y, learning_rate)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.forward(X)\n",
    "        return self.output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f7a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Create an MLP object\n",
    "mlp = MLP(input_size=2, hidden_size=4, output_size=1)\n",
    "\n",
    "# Train the MLP\n",
    "mlp.train(X, y, epochs=10000, learning_rate=0.1)\n",
    "\n",
    "# Test the MLP\n",
    "predictions = mlp.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96b580f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99906576]\n",
      " [0.9999866 ]\n",
      " [0.99699844]\n",
      " [0.99995018]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33e3757d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.31771213  0.80840607 -0.34387517  0.50924612]\n",
      " [ 2.06458092  1.73093796 -1.17949759  0.46595452]\n",
      " [-0.34461268  1.77095402 -1.07589665  0.28492606]\n",
      " [-0.5562792   0.94660084  0.56094874 -1.02139304]\n",
      " [ 0.56108371 -1.13273055  0.59444631  1.01363462]\n",
      " [ 1.0540316  -1.35319818 -0.61147083  1.69883598]\n",
      " [ 1.27366367  0.45211117  0.87344654 -0.17615236]\n",
      " [-0.12806607 -1.49915707 -0.2686678  -0.68117001]\n",
      " [-0.64220631  2.50910649 -0.03547154 -1.5774031 ]\n",
      " [-0.39907123  0.48624918 -0.63709872  0.54964787]]\n"
     ]
    }
   ],
   "source": [
    "a=np.random.randn(10, 4)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea4a729d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.39907123  0.48624918 -0.63709872  0.54964787]\n"
     ]
    }
   ],
   "source": [
    "print(a[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e2ced19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a43e19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
